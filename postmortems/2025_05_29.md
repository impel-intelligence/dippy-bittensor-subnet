# Postmortem May 29 2025

# Subnet 11 Model Scoring Incident Postmortem - May 29 2025

On Thursday, May 29, 2025, miners reported inconsistencies with the scoring being created by the validation workers.
The validation workers are a select set of GPU provisioned machines that are responsible for scoring each model uploaded to the subnet,

This post outlines the events that caused this incident, the architecture we had in place, what failed, what worked and why, and the changes we're making based on what we learned over the last few days.

## Intended Design

Our validation workers have a generally simple system: download a model, run it against a dockerized scoring application, and process the score to be assigned for each model. Of important note here is that while the scoring is containerized, the volumes mapped rely on specific configurations given the relative disk usage required per model (60gb+ per model) and the GPUs required. To also optimize loading of models, workers first download the models before launching the scoring code, attaching it as a volume to allow for easy access and management.

To set up the mapping for each worker, we use `direnv` to manage worker specific environment variables, allowing each worker to have their own alloted resources for scoring. 

## Initial Incident

Miners submitted lower spec models that were receiving scores that were higher than expected. Initially, there was no method to immediately identify which models were impacted, and scoring was fully paused for investigation. Over the next period of time, we found a number of configurations that contributed to the scoring issues.

## Response and Recovery

There were two main issues that led to inconsistent scoring:
1. An older docker image used for scoring
2. Inconsistent volume mappings

For the first, this meant that the code used for scoring was using an older methodology, thus leading for inconsistent scores. For the second, the wrong volume mappings meant that due to an erroneous configuration, downloaded models were not the same model that was actually loaded for scoring. Specifically, a faulty syntax as seen here :
```python
model_dir = "/path/to/model/{model_id}"
```
meant that the directory was loading the same path, instead of the intended line:
```python
model_dir = f"/path/to/model/{model_id}"
```


## What Worked Well
- Workers were paused and the issue was contained immediately
- Existing workers had traceability to identify issues

## What Didn't Work Well
- Lack of version control for configuration (known given there are secrets configured)

## Remediation Steps

Based on this incident, we are implementing the following changes:

1. Scoring reset
   - Allows equal ground for re assessing scored models  
2. Updated configuration loading  
   - In the case of additional configuration issues, errors will bubble up faster

## Future Development

Given that the current scoring subnet will be sunset in favor of a newer subnet architecture, the priority of this issue was lowered. 
